{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adcb339-5332-4345-a0cb-ad4f84cfef08",
   "metadata": {},
   "source": [
    "# Coordinate Parsing Example\n",
    "\n",
    "Here is a quick example of showing coordinate parsing on a small dataset (<10k rows).\n",
    "This was made from trips randomly constructed from GPS points in the Kansas City (USA)\n",
    "metro area.\n",
    "\n",
    "Some are small, some are large. I have no idea. it was grabbing a bunch of random\n",
    "coordinates and finding routes between them.\n",
    "\n",
    "Below is an example of using ``rapidgeo`` to create polylines out of the coordinate arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5f41f0-a566-456b-8239-b8d33c0e5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rapidgeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d5341d-2c91-4a81-8b68-c5db181f9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RapidGeo version: 0.2.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"RapidGeo version: {rapidgeo.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a36fa06-335c-43a2-b743-0cf61a77d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7,496 KC Metro routes\n",
      "\n",
      "Original data type: <class 'numpy.ndarray'>\n",
      "contiguous coords type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>coordinates_array</th>\n",
       "      <th>gps_points</th>\n",
       "      <th>contiguous_coords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kc_000000</td>\n",
       "      <td>[[-94.6821126, 38.9578505], [-94.6820774999999...</td>\n",
       "      <td>13</td>\n",
       "      <td>[[-94.6821126, 38.9578505], [-94.6820774999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kc_000001</td>\n",
       "      <td>[[-95.217415, 38.9726943], [-95.2190526, 38.97...</td>\n",
       "      <td>12</td>\n",
       "      <td>[[-95.217415, 38.9726943], [-95.2190526, 38.97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kc_000002</td>\n",
       "      <td>[[-94.8391741, 38.7676072], [-94.8282483999999...</td>\n",
       "      <td>16</td>\n",
       "      <td>[[-94.8391741, 38.7676072], [-94.8282483999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kc_000003</td>\n",
       "      <td>[[-94.9115648, 39.0301498], [-94.9113505999999...</td>\n",
       "      <td>11</td>\n",
       "      <td>[[-94.9115648, 39.0301498], [-94.9113505999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kc_000004</td>\n",
       "      <td>[[-94.6948674, 38.78569299999999], [-94.694291...</td>\n",
       "      <td>13</td>\n",
       "      <td>[[-94.6948674, 38.78569299999999], [-94.694291...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_id                                  coordinates_array  gps_points  \\\n",
       "0  kc_000000  [[-94.6821126, 38.9578505], [-94.6820774999999...          13   \n",
       "1  kc_000001  [[-95.217415, 38.9726943], [-95.2190526, 38.97...          12   \n",
       "2  kc_000002  [[-94.8391741, 38.7676072], [-94.8282483999999...          16   \n",
       "3  kc_000003  [[-94.9115648, 39.0301498], [-94.9113505999999...          11   \n",
       "4  kc_000004  [[-94.6948674, 38.78569299999999], [-94.694291...          13   \n",
       "\n",
       "                                   contiguous_coords  \n",
       "0  [[-94.6821126, 38.9578505], [-94.6820774999999...  \n",
       "1  [[-95.217415, 38.9726943], [-95.2190526, 38.97...  \n",
       "2  [[-94.8391741, 38.7676072], [-94.8282483999999...  \n",
       "3  [[-94.9115648, 39.0301498], [-94.9113505999999...  \n",
       "4  [[-94.6948674, 38.78569299999999], [-94.694291...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_python_lists(coords_array):\n",
    "    return [[float(coord[0]), float(coord[1])] for coord in coords_array]\n",
    "\n",
    "def to_flat_numpy(coords_array):\n",
    "    return np.array(coords_array).flatten()\n",
    "\n",
    "def to_contiguous_2d(coords_array):\n",
    "  return np.array([[float(coord[0]), float(coord[1])] for coord in coords_array], dtype=np.float64)\n",
    "\n",
    "\n",
    "df = pd.read_parquet('kc_metro_combined.parquet', columns=('trip_id', 'coordinates_array', 'gps_points'))\n",
    "print(f\"Loaded {len(df):,} KC Metro routes\")\n",
    "\n",
    "df['contiguous_coords'] = df['coordinates_array'].apply(to_contiguous_2d)\n",
    "\n",
    "coords = df['coordinates_array'].iloc[0]\n",
    "print(f\"\\nOriginal data type: {type(coords)}\")\n",
    "\n",
    "coords = df['contiguous_coords'].iloc[0]\n",
    "print(f\"contiguous coords type: {type(coords)}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443b6be5-b14a-498e-8c1e-7ded0cbc0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coords(df_name, coords):\n",
    "    \"\"\"\n",
    "    Test Performance time creating polylines\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df_name['polyline'] = df_name[coords].apply(\n",
    "        lambda coords: rapidgeo.polyline.encode(\n",
    "            rapidgeo.formats.coords_to_lnglat(coords)\n",
    "        )\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    routes_per_sec = len(df) / total_time\n",
    "    \n",
    "    print(f\"\\t\\tProcessed {len(df_name):,} routes in {total_time:.4f} seconds\")\n",
    "    print(f\"\\t\\tRate: {routes_per_sec:.4f} routes/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea10c67-c71c-4c72-82ec-5bb0d4e64a11",
   "metadata": {},
   "source": [
    "## Processing original ndarray\n",
    "\n",
    "Out of parquet/pandas we get a numpy array. These are fast!  The polyline algorithm should just chug through this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be963731-b136-4454-9d8b-97e6f358b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native numpy array\n",
      "\t\tProcessed 7,496 routes in 0.0402 seconds\n",
      "\t\tRate: 186653.6620 routes/second\n",
      "\n",
      "contiguous_2d\n",
      "\t\tProcessed 7,496 routes in 0.0273 seconds\n",
      "\t\tRate: 274227.8984 routes/second\n"
     ]
    }
   ],
   "source": [
    "print(\"native numpy array\")\n",
    "test_coords(df, 'coordinates_array')\n",
    "\n",
    "print(\"\\ncontiguous_2d\")\n",
    "test_coords(df, 'contiguous_coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b65385-da2e-4d4d-ba90-d2ff41d689ab",
   "metadata": {},
   "source": [
    "## Python Lists\n",
    "\n",
    "Let's see what happens on this dataset in a list of lists\n",
    "\n",
    "It might surprise you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172a7769-cbee-41c3-adfd-ddc1c7546ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Data Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df['list_coords'] = df['coordinates_array'].apply(to_python_lists)\n",
    "\n",
    "coords = df['list_coords'].iloc[0]\n",
    "print(f\"List Data Type: {type(coords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00137b37-f880-4e80-8be4-5e44e79ad12a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Pretty interesting, right?\n",
    "\n",
    "Let's make some larger datasets to see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd61d96-45bb-442b-910b-7976f62a5087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 7,496 routes\n",
      "2x: 14,992 routes\n",
      "10x: 112,440 routes\n",
      "35x: 262,360 routes\n"
     ]
    }
   ],
   "source": [
    "df_2x = pd.concat([df] * 2, ignore_index=True)\n",
    "df_6x = pd.concat([df] * 5, ignore_index=True)\n",
    "df_10x = pd.concat([df] * 15, ignore_index=True)\n",
    "df_35x = pd.concat([df] * 35, ignore_index=True)\n",
    "\n",
    "print(f\"Original: {len(df):,} routes\")\n",
    "print(f\"2x: {len(df_2x):,} routes\")\n",
    "print(f\"10x: {len(df_10x):,} routes\")\n",
    "print(f\"35x: {len(df_35x):,} routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8125725e-891f-40e7-a83d-af1a18443901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing: 14,992 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 14,992 routes in 0.8098 seconds\n",
      "\t\tRate: 9256.7466 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 14,992 routes in 0.3781 seconds\n",
      "\t\tRate: 19824.5972 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 14,992 routes in 0.3459 seconds\n",
      "\t\tRate: 21671.5821 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 37,480 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 37,480 routes in 2.0028 seconds\n",
      "\t\tRate: 3742.8497 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 37,480 routes in 1.0121 seconds\n",
      "\t\tRate: 7406.5958 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 37,480 routes in 0.8655 seconds\n",
      "\t\tRate: 8660.9901 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 112,440 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 112,440 routes in 6.1878 seconds\n",
      "\t\tRate: 1211.4184 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 112,440 routes in 2.8069 seconds\n",
      "\t\tRate: 2670.5847 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 112,440 routes in 2.6264 seconds\n",
      "\t\tRate: 2854.0772 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 262,360 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 262,360 routes in 14.4903 seconds\n",
      "\t\tRate: 517.3101 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 262,360 routes in 6.6348 seconds\n",
      "\t\tRate: 1129.7921 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 262,360 routes in 6.1224 seconds\n",
      "\t\tRate: 1224.3524 routes/second\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "for dataframe in (df_2x, df_6x, df_10x, df_35x):\n",
    "    print(f\"Testing: {len(dataframe):,} Records!\")\n",
    "    print(\"\\tNumpy Array\")\n",
    "    test_coords(dataframe, 'coordinates_array')\n",
    "\n",
    "    print(\"\\n\\tList\")\n",
    "    test_coords(dataframe, 'list_coords')\n",
    "\n",
    "    print(\"\\n\\tcontiguous_2d\")\n",
    "    test_coords(dataframe, 'contiguous_coords')\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14de54d-b100-439b-b7dd-2dc9474d6737",
   "metadata": {},
   "source": [
    "# Batch encoding polylines\n",
    "\n",
    "We've been a bit unfair to the process so far, as all of the benchmarks are taking into account \n",
    "BOTH coordinate parsing *and* creating polylines.\n",
    "\n",
    "Let's get out of a single CPU and show what ``encode_batch()`` can do with this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb3a93b-f58e-486b-97c3-882caf1ba6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_coords(df_name, coords):\n",
    "  \"\"\"\n",
    "  Test Performance time creating polylines\n",
    "  \"\"\"\n",
    "  start_time = time.time()\n",
    "  df_name['polyline'] = rapidgeo.polyline.encode_column(df_name[coords])\n",
    "  end_time = time.time()\n",
    "\n",
    "  total_time = end_time - start_time\n",
    "  routes_per_sec = len(df) / total_time\n",
    "\n",
    "  print(f\"\\t\\tProcessed {len(df_name):,} routes in {total_time:.4f} seconds\")\n",
    "  print(f\"\\t\\tRate: {routes_per_sec:.4f} routes/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb6534d-ca76-43e5-a6b5-134a34409b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing: 14,992 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 14,992 routes in 0.0451 seconds\n",
      "\t\tRate: 166326.3456 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 14,992 routes in 0.0221 seconds\n",
      "\t\tRate: 339017.7139 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 14,992 routes in 0.0182 seconds\n",
      "\t\tRate: 412659.1782 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 37,480 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 37,480 routes in 0.1096 seconds\n",
      "\t\tRate: 68390.2506 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 37,480 routes in 0.0526 seconds\n",
      "\t\tRate: 142576.9685 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 37,480 routes in 0.0459 seconds\n",
      "\t\tRate: 163374.8144 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 112,440 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 112,440 routes in 0.3402 seconds\n",
      "\t\tRate: 22033.3135 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 112,440 routes in 0.1650 seconds\n",
      "\t\tRate: 45440.7535 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 112,440 routes in 0.1519 seconds\n",
      "\t\tRate: 49350.2521 routes/second\n",
      "============================================================\n",
      "\n",
      "Testing: 262,360 Records!\n",
      "\tNumpy Array\n",
      "\t\tProcessed 262,360 routes in 0.8287 seconds\n",
      "\t\tRate: 9045.1409 routes/second\n",
      "\n",
      "\tList\n",
      "\t\tProcessed 262,360 routes in 0.4223 seconds\n",
      "\t\tRate: 17749.2378 routes/second\n",
      "\n",
      "\tcontiguous_2d\n",
      "\t\tProcessed 262,360 routes in 0.3734 seconds\n",
      "\t\tRate: 20074.0780 routes/second\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "for dataframe in (df_2x, df_6x, df_10x, df_35x):\n",
    "    print(f\"Testing: {len(dataframe):,} Records!\")\n",
    "    print(\"\\tNumpy Array\")\n",
    "    test_batch_coords(dataframe, 'coordinates_array')\n",
    "\n",
    "    print(\"\\n\\tList\")\n",
    "    test_batch_coords(dataframe, 'list_coords')\n",
    "\n",
    "    print(\"\\n\\tcontiguous_2d\")\n",
    "    test_batch_coords(dataframe, 'contiguous_coords')\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a327ab-eb75-4b63-a408-e60afaab464f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We have a dropoff as we get more results. If I were in a cluster, I could throw more computers at it.\n",
    "\n",
    "But on a single machine, we can chunk and try to get the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53c2302-f5e3-4cfd-9d3e-9df870d29f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE WORKFLOW COMPARISON\n",
      "Testing full pipeline: processing + DataFrame assignment\n",
      "\n",
      "HEAD-TO-HEAD: Original (7.5k)\n",
      "1️MONOLITHIC (original approach):\n",
      "\t\tMonolithic: 7,496 records in 0.009s = 811347 routes/sec\n",
      "2️CHUNKED (optimized approach):\n",
      "\n",
      "=== Chunked Processing + DF Assignment:  ===\n",
      "Total records: 7,496 | Chunk size: 10,000\n",
      "Processing in 1 chunks...\n",
      "\t\tChunk 1/1: 7,496 records in 0.009s = 843836 routes/sec\n",
      "\n",
      "COMPLETE WORKFLOW RESULTS:\n",
      "\t\tProcessing time: 0.009s\n",
      "\t\tDataFrame assignment time: 0.002s\n",
      "\t\tTotal workflow time: 0.011s\n",
      "\t\tProcessing rate: 843836 routes/second\n",
      "\t\tComplete workflow rate: 686579 routes/second\n",
      "\t\tAssignment overhead:  17.6%\n",
      "Results match!\n",
      "\n",
      "HEAD-TO-HEAD: 2x (15k)\n",
      "1️MONOLITHIC (original approach):\n",
      "\t\tMonolithic: 14,992 records in 0.016s = 931074 routes/sec\n",
      "2️CHUNKED (optimized approach):\n",
      "\n",
      "=== Chunked Processing + DF Assignment:  ===\n",
      "Total records: 14,992 | Chunk size: 10,000\n",
      "Processing in 2 chunks...\n",
      "\t\tChunk 1/2: 10,000 records in 0.011s = 923205 routes/sec\n",
      "\t\tChunk 2/2: 4,992 records in 0.007s = 720855 routes/sec\n",
      "\n",
      "COMPLETE WORKFLOW RESULTS:\n",
      "\t\tProcessing time: 0.018s\n",
      "\t\tDataFrame assignment time: 0.001s\n",
      "\t\tTotal workflow time: 0.020s\n",
      "\t\tProcessing rate: 844290 routes/second\n",
      "\t\tComplete workflow rate: 764929 routes/second\n",
      "\t\tAssignment overhead:  7.5%\n",
      "Results match!\n",
      "\n",
      "HEAD-TO-HEAD: 5x (37k)\n",
      "1️MONOLITHIC (original approach):\n",
      "\t\tMonolithic: 37,480 records in 0.044s = 849078 routes/sec\n",
      "2️CHUNKED (optimized approach):\n",
      "\n",
      "=== Chunked Processing + DF Assignment:  ===\n",
      "Total records: 37,480 | Chunk size: 10,000\n",
      "Processing in 4 chunks...\n",
      "\t\tChunk 1/4: 10,000 records in 0.010s = 1006142 routes/sec\n",
      "\t\tChunk 2/4: 10,000 records in 0.010s = 974355 routes/sec\n",
      "\t\tChunk 3/4: 10,000 records in 0.010s = 988430 routes/sec\n",
      "\t\tChunk 4/4: 7,480 records in 0.007s = 1036589 routes/sec\n",
      "\n",
      "COMPLETE WORKFLOW RESULTS:\n",
      "\t\tProcessing time: 0.038s\n",
      "\t\tDataFrame assignment time: 0.004s\n",
      "\t\tTotal workflow time: 0.042s\n",
      "\t\tProcessing rate: 998530 routes/second\n",
      "\t\tComplete workflow rate: 894111 routes/second\n",
      "\t\tAssignment overhead:  8.9%\n",
      "Results match!\n",
      "\n",
      "HEAD-TO-HEAD: 15x (112k)\n",
      "1️MONOLITHIC (original approach):\n",
      "\t\tMonolithic: 112,440 records in 0.140s = 801973 routes/sec\n",
      "2️CHUNKED (optimized approach):\n",
      "\n",
      "=== Chunked Processing + DF Assignment:  ===\n",
      "Total records: 112,440 | Chunk size: 10,000\n",
      "Processing in 12 chunks...\n",
      "\t\tChunk 1/12: 10,000 records in 0.010s = 955727 routes/sec\n",
      "\t\tChunk 2/12: 10,000 records in 0.010s = 999572 routes/sec\n",
      "\t\tChunk 3/12: 10,000 records in 0.011s = 939332 routes/sec\n",
      "\t\tChunk 4/12: 10,000 records in 0.011s = 935268 routes/sec\n",
      "\t\tChunk 5/12: 10,000 records in 0.011s = 948423 routes/sec\n",
      "\t\tChunk 6/12: 10,000 records in 0.012s = 857854 routes/sec\n",
      "\t\tChunk 7/12: 10,000 records in 0.011s = 910499 routes/sec\n",
      "\t\tChunk 8/12: 10,000 records in 0.010s = 990110 routes/sec\n",
      "\t\tChunk 9/12: 10,000 records in 0.011s = 930826 routes/sec\n",
      "\t\tChunk 10/12: 10,000 records in 0.011s = 876058 routes/sec\n",
      "\t\tChunk 11/12: 10,000 records in 0.011s = 911904 routes/sec\n",
      "\t\tChunk 12/12: 2,440 records in 0.002s = 976816 routes/sec\n",
      "\n",
      "COMPLETE WORKFLOW RESULTS:\n",
      "\t\tProcessing time: 0.121s\n",
      "\t\tDataFrame assignment time: 0.010s\n",
      "\t\tTotal workflow time: 0.134s\n",
      "\t\tProcessing rate: 931480 routes/second\n",
      "\t\tComplete workflow rate: 838503 routes/second\n",
      "\t\tAssignment overhead:  7.3%\n",
      "Results match!\n",
      "\n",
      "HEAD-TO-HEAD: 35x (262k)\n",
      "1️MONOLITHIC (original approach):\n",
      "\t\tMonolithic: 262,360 records in 0.349s = 750969 routes/sec\n",
      "2️CHUNKED (optimized approach):\n",
      "\n",
      "=== Chunked Processing + DF Assignment:  ===\n",
      "Total records: 262,360 | Chunk size: 10,000\n",
      "Processing in 27 chunks...\n",
      "\t\tChunk 1/27: 10,000 records in 0.012s = 829586 routes/sec\n",
      "\t\tChunk 2/27: 10,000 records in 0.011s = 940743 routes/sec\n",
      "\t\tChunk 3/27: 10,000 records in 0.011s = 892956 routes/sec\n",
      "\t\tChunk 4/27: 10,000 records in 0.012s = 827540 routes/sec\n",
      "\t\tChunk 5/27: 10,000 records in 0.011s = 937317 routes/sec\n",
      "\t\tChunk 6/27: 10,000 records in 0.013s = 783821 routes/sec\n",
      "\t\tChunk 7/27: 10,000 records in 0.010s = 981330 routes/sec\n",
      "\t\tChunk 8/27: 10,000 records in 0.012s = 862227 routes/sec\n",
      "\t\tChunk 9/27: 10,000 records in 0.011s = 910084 routes/sec\n",
      "\t\tChunk 10/27: 10,000 records in 0.011s = 878664 routes/sec\n",
      "\t\tChunk 11/27: 10,000 records in 0.013s = 796624 routes/sec\n",
      "\t\tChunk 12/27: 10,000 records in 0.011s = 944727 routes/sec\n",
      "\t\tChunk 13/27: 10,000 records in 0.010s = 988919 routes/sec\n",
      "\t\tChunk 14/27: 10,000 records in 0.011s = 935915 routes/sec\n",
      "\t\tChunk 15/27: 10,000 records in 0.012s = 836535 routes/sec\n",
      "\t\tChunk 16/27: 10,000 records in 0.010s = 967589 routes/sec\n",
      "\t\tChunk 17/27: 10,000 records in 0.010s = 960806 routes/sec\n",
      "\t\tChunk 18/27: 10,000 records in 0.011s = 926590 routes/sec\n",
      "\t\tChunk 19/27: 10,000 records in 0.011s = 945174 routes/sec\n",
      "\t\tChunk 20/27: 10,000 records in 0.011s = 924140 routes/sec\n",
      "\t\tChunk 21/27: 10,000 records in 0.011s = 887777 routes/sec\n",
      "\t\tChunk 22/27: 10,000 records in 0.011s = 913334 routes/sec\n",
      "\t\tChunk 23/27: 10,000 records in 0.011s = 914649 routes/sec\n",
      "\t\tChunk 24/27: 10,000 records in 0.011s = 934060 routes/sec\n",
      "\t\tChunk 25/27: 10,000 records in 0.010s = 986361 routes/sec\n",
      "\t\tChunk 26/27: 10,000 records in 0.011s = 918032 routes/sec\n",
      "\t\tChunk 27/27: 2,360 records in 0.003s = 836592 routes/sec\n",
      "\n",
      "COMPLETE WORKFLOW RESULTS:\n",
      "\t\tProcessing time: 0.290s\n",
      "\t\tDataFrame assignment time: 0.024s\n",
      "\t\tTotal workflow time: 0.320s\n",
      "\t\tProcessing rate: 904441 routes/second\n",
      "\t\tComplete workflow rate: 820354 routes/second\n",
      "\t\tAssignment overhead:  7.5%\n",
      "Results match!\n"
     ]
    }
   ],
   "source": [
    "def chunked_encode_with_assignment(df_sample, coord_column, chunk_size=10000, description=\"\"):\n",
    "    \"\"\"\n",
    "    FULL TEST: Process large datasets in chunks AND assign back to DataFrame\n",
    "    This is the true apples-to-apples comparison with the original monolithic approach\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Chunked Processing + DF Assignment: {description} ===\")\n",
    "    print(f\"Total records: {len(df_sample):,} | Chunk size: {chunk_size:,}\")\n",
    "\n",
    "    coord_list = df_sample[coord_column].tolist()\n",
    "    total_chunks = (len(coord_list) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    all_polylines = []\n",
    "    total_time = 0\n",
    "\n",
    "    print(f\"Processing in {total_chunks} chunks...\")\n",
    "\n",
    "    # TIME THE ENTIRE WORKFLOW INCLUDING DATAFRAME OPERATIONS\n",
    "    workflow_start = time.time()\n",
    "\n",
    "    for i in range(0, len(coord_list), chunk_size):\n",
    "        chunk = coord_list[i:i + chunk_size]\n",
    "        actual_chunk_size = len(chunk)\n",
    "\n",
    "        # Process chunk at peak performance\n",
    "        start_time = time.time()\n",
    "        chunk_polylines = rapidgeo.polyline.encode_column(chunk)\n",
    "        chunk_time = time.time() - start_time\n",
    "\n",
    "        total_time += chunk_time\n",
    "        all_polylines.extend(chunk_polylines)\n",
    "\n",
    "        chunk_rate = actual_chunk_size / chunk_time\n",
    "        print(f\"\\t\\tChunk {(i//chunk_size)+1}/{total_chunks}: {actual_chunk_size:,} records in {chunk_time:.3f}s = {chunk_rate:.0f} routes/sec\")\n",
    "\n",
    "    # ASSIGN BACK TO DATAFRAME (this is what we were missing!)\n",
    "    assignment_start = time.time()\n",
    "    df_sample['polyline_chunked'] = all_polylines\n",
    "    assignment_time = time.time() - assignment_start\n",
    "\n",
    "    total_workflow_time = time.time() - workflow_start\n",
    "\n",
    "    # Overall stats INCLUDING DataFrame assignment\n",
    "    processing_rate = len(df_sample) / total_time\n",
    "    workflow_rate = len(df_sample) / total_workflow_time\n",
    "\n",
    "    print(f\"\\nCOMPLETE WORKFLOW RESULTS:\")\n",
    "    print(f\"\\t\\tProcessing time: {total_time:.3f}s\")\n",
    "    print(f\"\\t\\tDataFrame assignment time: {assignment_time:.3f}s\")\n",
    "    print(f\"\\t\\tTotal workflow time: {total_workflow_time:.3f}s\")\n",
    "    print(f\"\\t\\tProcessing rate: {processing_rate:.0f} routes/second\")\n",
    "    print(f\"\\t\\tComplete workflow rate: {workflow_rate:.0f} routes/second\")\n",
    "    print(f\"\\t\\tAssignment overhead:  {(assignment_time/total_workflow_time)*100:.1f}%\")\n",
    "\n",
    "    return all_polylines\n",
    "\n",
    "def compare_chunked_vs_monolithic(df_sample, coord_column, description=\"\"):\n",
    "    \"\"\"\n",
    "    Direct comparison: chunked vs monolithic with full DataFrame  assignment\n",
    "    \"\"\"\n",
    "    print(f\"\\nHEAD-TO-HEAD: {description}\")\n",
    "\n",
    "    # Test 1: Monolithic approach (original)\n",
    "    print(\"1️MONOLITHIC (original approach):\")\n",
    "    start_time = time.time()\n",
    "    df_sample['polyline_mono'] = rapidgeo.polyline.encode_column(df_sample[coord_column])\n",
    "    mono_time = time.time() - start_time\n",
    "    mono_rate = len(df_sample) / mono_time\n",
    "    print(f\"\\t\\tMonolithic: {len(df_sample):,} records in {mono_time:.3f}s = {mono_rate:.0f} routes/sec\")\n",
    "\n",
    "    # Test 2: Chunked approach  \n",
    "    print(\"2️CHUNKED (optimized approach):\")\n",
    "    chunked_polylines = chunked_encode_with_assignment(df_sample, coord_column, chunk_size=10000, description=\"\")\n",
    "\n",
    "    # Verify results are identical\n",
    "    if df_sample['polyline_mono'].equals(df_sample['polyline_chunked']):\n",
    "        print(\"Results match!\")\n",
    "    else:\n",
    "        print(\"Results differ!\")\n",
    "\n",
    "    return mono_rate\n",
    "\n",
    "# Run the COMPLETE comparison\n",
    "print(\"COMPLETE WORKFLOW COMPARISON\")\n",
    "print(\"Testing full pipeline: processing + DataFrame assignment\")\n",
    "\n",
    "datasets = [\n",
    "    (df, \"Original (7.5k)\"),\n",
    "    (df_2x, \"2x (15k)\"),\n",
    "    (df_6x, \"5x (37k)\"),\n",
    "    (df_10x, \"15x (112k)\"),\n",
    "    (df_35x, \"35x (262k)\")\n",
    "]\n",
    "\n",
    "for dataset, name in datasets:\n",
    "    mono_rate = compare_chunked_vs_monolithic(dataset.copy(), 'contiguous_coords', description=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fa432-e1bc-4bce-8979-7708f0e5dae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
